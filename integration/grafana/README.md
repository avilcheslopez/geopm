# Grafana

This directory contains a JSON file for the GEOPM Grafana dashboard that
visualizes metrics generated by the `geopmexporter(1)` Prometheus data source.
Additionally the directory contains a script to enable the monitoring of jobs
launched using the clush (cluster shell) command.


## Importing the Grafana dashboard

Follow the steps documented in the official Grafana user guide on [importing
dashboards](https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/import-dashboards)
to import the GEOPM Report JSON file in this directory.  This will enable the
visualization of data collected by the `geopmexporter(1)`.


## Data Source for the Visuals

Grafana is capable of sourcing telemetry data from Prometheus clients running on
remote hosts. One such Prometheus exporter is the `geopmexporter(1)`
<https://geopm.github.io/geopmexporter.1.html> which leverages
`geopm.stats.collector` to summarize the metrics. The dashboard file -
`GEOPM_Report.grafana.dashboard.json`, in this directory, relies on
`geopmexporter(1)` for its data source.


## Use case: Monitoring User Jobs with Clush

The `geopmexporter(1)` is typically deployed system-wide with either a [systemd
service](https://geopm.github.io/geopmexporter.1.html#systemd-service) or as a
[kubernetes service](https://github.com/cmcantalupo/geopm/tree/prometheus/integration/k8#deploying-prometheus-client-in-kubernetes).
This documentation is for an alternative `geopmexporter(1)` deployment scenario
where an unprivileged user of an HPC system with a resource manager would like
to monitor the compute resources granted to the user only while they are
allocated.  This methodology should not be applied for system-wide monitoring by
an administrator, but rather for job monitoring by an end user of a PBS system
or similar where the geopmexporter is not deployed system-wide.

A script is provided called `clush_prometheus.py` that can be used to:

- Assist with the initial one-time configuration of the Grafana server
- Monitor jobs submitted to a batch system
- Review data collected from previously monitored jobs

The script runs the Prometheus and Grafana server on the head node of a
system. Optionally, the script may also deploy and connect to `geopmexporter(1)`
clients running on the allocated nodes of a user's job with the clush command.


### Requirements

A user installation of Grafana and Prometheus is required.  We recommend
following the [Linux installation
instructions](https://grafana.com/grafana/download?platform=linux) to `wget` the
Grafana archive for Linux.  The expanded archive is the `GRAFANA_DIR` provided
to the `clush_prometheus.py` script CLI.  To install Prometheus download
the [archive for the latest stable Linux](https://prometheus.io/download/).  The
expanded archive is the `PROMETHEUS_DIR` provided to the
`clush_prometheus.py` script CLI.

The user must have an [install
ClusterShell]((https://clustershell.readthedocs.io/en/latest/install.html)) or
have access to `clush` through a system install.

Until the `geopmexporter(1)` is available in a stable release, the user must
build and install a development snapshot of GEOPM.

```bash
    # Install latest development snapshot of GEOPM
    GEOPM_PREFIX=... # target installation directory
    wget https://raw.githubusercontent.com/geopm/geopm/refs/heads/dev/geopmdpy/install_user.sh
    chmod a+x install_user.sh
    ./install_user.sh --prefix=$GEOPM_PREFIX --enable-levelzero
```


### Configuring ports

There are three ports used by the script to communicate:

- Prometheus server port opened on the head node (default 9090)
- Grafana server port opened on the head node (default 3000)
- Prometheus client port opened on the allocated nodes (default 8000)

To avoid conflicts, or firewall limitations, the user may override the defaults
using command line arguments to `clush_prometheus.py`.

### Configuring Grafana Server

Before deploying the GEOPM Prometheus exporter across the system for the first
time, the Grafana server needs to be configured on the head node by setting the
administrator password and uploading the GEOPM Power Dashboard though the web
GUI.

To set the administrator password for your Grafana instance with the `grafana-cli`
tool run the following command inside of the Grafana software directory:

```bash
    cd $GRAFANA_DIR/
     ./bin/grafana-cli admin reset-admin-password $GRAFANA_ADMIN_PASSWORD
```

To add the GEOPM Prometheus dashboard to your Grafana configuration the Grafana
and Prometheus servers must be running.  This is done by executing the
`clush_prometheus.py` script without specifying the `--pbs-jobid/--hostfile` option.  This
will bring up the Prometheus server and Grafana server on the head node.

Then navigate to the Grafana web GUI in a web browser to import the GEOPM
Dashboard.  The Grafana server will be running an http (not https) server on the
head node URL on port 3000 (port can be modified with the the --graf-port option
to `clush_prometheus.py`).  For example if the head node hostname is
`login.cluster.acme.com` then the Grafana server is reached through the URL
`http://login.cluster.acme.com:3000`.  Login into the Grafana web page with the
`admin` user credentials set previously using the `grafana-cli` tool.  Once
logged in, add the http Prometheus data source using port 9090 (unless overridden
with the --prom-port option to `clush_prometheus.py`),
e.g.`http://localhost:9090`.  Next, import the [GEOPM Power Report Grafana
dashboard configuration
file](https://raw.githubusercontent.com/geopm/geopm/refs/heads/dev/integration/grafana/GEOPM_Report.grafana.dashboard.json)
using the Prometheus data source by drag-and-drop or copy-paste.


### Monitor the user job using the GEOPM Prometheus & Grafana framework

Now that you have configured Grafana and Prometheus, you can launch the Prometheus
client exporters across the allocated nodes of a job, and then use the Grafana Web GUI
(running on port `GRAFANA_SERVER_PORT` to monitor the aggregated telemetry.

First submit a job to the PBS queue using qsub to obtain a PBS Job ID.  Use the
PBS Job ID (`JOBID`) of the submitted job when launching the servers:

```bash
    JOBID=$(qsub ...)
    ./clush_prometheus.py --geopm-prefix GEOPM_PREFIX --pbs-jobid JOBID PROMETHEUS_DIR GRAFANA_DIR

```

The script will print the path to three logs that can be used to monitor the
Grafana server, Prometheus server, and Prometheus clients.  The servers running
on the login node will be terminated by the script when the allocation
completes.


### Review previously collected metrics

Simply run the `clush_prometheus.py` script without the `--pbs-jobid` option to
enable review of previously collected metrics.  When review is complete, press
enter in the terminal to bring down the Prometheus and Grafana servers.  While
the servers are up, the Grafana web interface providing the GEOPM dashboard can
be used to inspect any historical data collected from previous PBS jobs that
were monitored.
