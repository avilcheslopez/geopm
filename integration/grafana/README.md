# Grafana

This directory contains a Grafana dashboard configuration file to conveniently
visualize metrics generated by the `geopmexporter(1)` Prometheus data source.
Additionally the directory contains a script to enable the monitoring of jobs
launched using the `clush(1)` (cluster shell) command.

## Data Source for the Visuals

Grafana is capable of sourcing telemetry data from Prometheus clients running on
remote hosts. One such Prometheus exporter is the
[geopmexporter(1)](https://geopm.github.io/geopmexporter.1.html) which leverages
`geopm.stats.collector` to summarize the metrics. The dashboard file -
`GEOPM_Report.grafana.dashboard.json`, in this directory, relies on
`geopmexporter(1)` for its data source.  Before adding the GEOPM Grafana
Dashboard, first [add the Prometheus data
source](https://grafana.com/docs/grafana/latest/datasources/#add-a-data-source)
to your Grafana server.


## Importing the Grafana dashboard

Follow the steps documented in the official Grafana user guide on [importing
dashboards](https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/import-dashboards)
to import the GEOPM Report JSON file in this directory.  This will enable the
visualization of data collected by the `geopmexporter(1)`.


## Use case: Monitoring User Jobs with Clush

The `geopmexporter(1)` is typically deployed system-wide with either a [systemd
service](https://geopm.github.io/geopmexporter.1.html#systemd-service) or as a
[kubernetes service](https://github.com/cmcantalupo/geopm/tree/prometheus/integration/k8#deploying-prometheus-client-in-kubernetes).
This documentation is for an alternative `geopmexporter(1)` deployment scenario
where an unprivileged user of an HPC system with a resource manager would like
to monitor the compute resources granted to them only while they are
allocated.  This methodology should not be applied for system-wide monitoring by
an administrator, but rather for job monitoring by an end user of a PBS system
or similar where the `geopmexporter(1)` is not deployed system-wide.

To assist with this use case, a helper script is provided named
`clush_prometheus.py`, which can be used to:

- Assist with the initial one-time configuration of the Grafana server
- Monitor jobs submitted to a batch system
- Review data collected from previously monitored jobs

To perform these functions, the script runs the Prometheus and Grafana servers
on the head node of the system.  To generate data from user allocated compute
nodes, the script deploys the `geopmexporter(1)` using the `clush(1)` command.


### Requirements

Some software installation is required including GEOPM, Prometheus Server,
Grafana Server and ClusterShell tool.


#### Prometheus Install

To install Prometheus download the [archive for the latest stable
Linux](https://prometheus.io/download/).  The expanded archive is the
`PROMETHEUS_DIR` provided to the `clush_prometheus.py` script CLI.


#### Grafana Install

We recommend following the [Linux installation
instructions](https://grafana.com/grafana/download?platform=linux) to `wget` the
Grafana archive for Linux.  The expanded archive is the `GRAFANA_DIR` provided
to the `clush_prometheus.py` script CLI.


#### ClusterShell Install

The user must [install
ClusterShell]((https://clustershell.readthedocs.io/en/latest/install.html)) or
have access to `clush(1)` through a system install.  Unlike Prometheus and
Grafana, a user local install of `clush(1)` is not required.


#### GEOPM Install

The `geopmexporter(1)` is not available in a stable release (feature will be
available as of v3.2.0).  The user must build and install a development snapshot
of GEOPM if a version of GEOPM that supports `geopmexporter(1)` is not installed
system-wide e.g.:

```bash
    # Install latest development snapshot of GEOPM
    GEOPM_PREFIX=... # target installation directory
    wget https://raw.githubusercontent.com/geopm/geopm/refs/heads/dev/geopmdpy/install_user.sh
    chmod a+x install_user.sh
    ./install_user.sh --prefix=$GEOPM_PREFIX --enable-levelzero
```

When running `clush_prometheus.py` use the `--geopm-prefix=$GEOPM_PREFIX` option
unless the system install supports `geopmexporter(1)`.


### Configuring ports

There are three ports used by the script to communicate:

- Prometheus server port opened on the head node (default 9090)
- Grafana server port opened on the head node (default 3000)
- Prometheus client port opened on the allocated nodes (default 8000)

To avoid conflicts, or firewall limitations, the user may override the defaults
using command line arguments to `clush_prometheus.py`.


### Configuring Grafana Server

Before deploying the GEOPM Prometheus exporter across the system for the first
time, the Grafana server needs to be configured on the head node by setting the
administrator password and uploading the GEOPM Report Dashboard though the web
GUI.

To set the administrator password for your Grafana instance with the `grafana-cli`
tool run the following command inside of the Grafana software directory:

```bash
    cd $GRAFANA_DIR/
     ./bin/grafana-cli admin reset-admin-password --password-from-stdin
```

To add the GEOPM Prometheus dashboard to your Grafana configuration the Grafana
and Prometheus servers must be running.  This is done by executing the
`clush_prometheus.py` script without specifying the `--pbs-jobid`/`--hostfile`
option.

```bash
    clush_prometheus.py --pbs-jobid=JOBID PROMETHEUS_DIR GRAFANA_DIR
```
This will bring up the Prometheus server and Grafana server on the head
node.

Then navigate to the Grafana web GUI in a web browser to add the Prometheus Data
Source and then import the GEOPM Dashboard.  The Grafana server will be running
an http (not https) server on the head node URL on port 3000 (or the port
specified to `clush_prometheus.py` via the `--graf-port` option).  For example,
if the head node hostname is `login.cluster.acme.com` then the Grafana server is
reached through the URL `http://login.cluster.acme.com:3000`.  Login with the
button in the upper right of the Grafana web page by submitting the `admin` user
credentials set previously using the `grafana-cli` tool.  Once logged in, add
the http Prometheus data source using port 9090 (unless overridden with the
`--prom-port` option to `clush_prometheus.py`), e.g.`http://localhost:9090`.
Next, import the [GEOPM Power Report Grafana dashboard configuration
file](https://raw.githubusercontent.com/geopm/geopm/refs/heads/dev/integration/grafana/GEOPM_Report.grafana.dashboard.json)
using the Prometheus data source by drag-and-drop or copy-paste.

Bring down the Grafana and Prometheus servers and print the logs by pressing
ENTER at the prompt.  Note that pressing Control-C will have the same effect,
but the logs will be suppressed.  The Prometheus server, Grafana server, and
optional ClusterShell command will all be running as background processes while
the `clush_promethus.py` script is running.  These processes will be sent SIGINT
to bring them down upon the termination of `clush_prometheus.py`.


### Monitor the user job with the GEOPM Prometheus & Grafana framework

Now that you have configured Grafana and Prometheus, you can launch the
Prometheus client exporters across the allocated nodes of a job, and then use
the Grafana Web GUI to monitor the aggregated telemetry.

The `geopmexporter(1)` Prometheus client is launched using the `clush(1)`
command line tool configured with the `--hostfile` option.  The user can either
run `clush_prometheus.py --hostfile HOSTFILE` to have the option forwarded to
`clush(1)`, or derive the hostfile from the resource manager.

For the PBS resource manager this can be done by running `clush_prometheus.py --pbs-jobid JOBID`
to derive a temporary hostfile based on the PBS jobid.  First submit a job to
the PBS queue using `qsub(1)` to obtain a PBS Job ID.  Use the PBS Job ID
(`JOBID`) of the submitted job when launching the servers:

```bash
    JOBID=$(qsub ...)
    ./clush_prometheus.py --pbs-jobid JOBID PROMETHEUS_DIR GRAFANA_DIR

```

The script will print the path to three logs that can be used to monitor the
Grafana server, Prometheus server, and Prometheus clients.  The servers running
on the login node will be terminated by the script when the allocation
completes.

When using the `--pbs-jobid` option the ID provided may either be a job that is
queuing, or a job that is running.  If the job is queuing `clush_prometheus.py`
will poll `qstat(1)` every 15 seconds until the allocation is granted.

The `clush_prometheus.py` script will print an error from the `qstat(1)` command
and the script will exit with a non-zero return code prior if the user
specified Job ID is invalid or points to an already completed job.  The script
may also terminate early due to a port conflict if a `geopmexporter(1)` is
already running on the nodes of an allocation prior to starting the
`clush_prometheus.py` script.  These errors in using the `--pbs-jobid` option
terminate the `clush_promethus.py` script before it starts the Prometheus or
Grafana servers.


### Review previously collected metrics

Simply run the `clush_prometheus.py` script without the `--pbs-jobid` option to
launch Grafana and Prometheus servers that present previously collected metrics.
When review is complete, press enter in the terminal to bring down the
Prometheus and Grafana servers.  While the servers are up, the Grafana web
interface providing the GEOPM dashboard can be used to inspect any historical
data collected from previous PBS jobs that were monitored.
